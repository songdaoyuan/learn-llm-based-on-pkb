# 0x03 知识库的搭建

## 理解词向量及向量知识库

### 一、词向量

1. 什么是词向量

    在机器学习和自然语言处理（NLP）中，词向量（Embeddings）是一种将非结构化数据，如单词、句子或者整个文档，转化为实数向量的技术。

    ![alt text](../figure/0x03_Figure_00_Embeddings.png)

    嵌入背后的主要想法是，相似或相关的对象在嵌入空间（Embedding Space）中的距离应该很近。

    ![alt text](../figure/0x03_Figure_01_EmbeddingsSpace.png)

    我们可以使用词嵌入（word embeddings）来表示文本数据。在词嵌入中，每个单词被转换为一个向量，这个向量捕获了这个单词的语义信息。含义相似的单词在嵌入空间中的位置将会非常接近。

2. 词向量的优势

    RAG（Retrieval Augmented Generation，检索增强生成）中词向量的两大优势：

    * 检索效率：词向量包含语义信息，可通过计算相似度指标（如点积、余弦距离）直接获取语义层面的匹配度，优于仅依赖关键词匹配的文字检索。
    * 跨模态能力：词向量能将文字、声音、图像等多种媒介通过向量模型统一映射，实现跨模态关联和查询，而传统数据库难以实现这一点。

### 二、向量数据库

1. 向量数据库和它的优势

    向量数据库是专为存储和检索向量数据（如embedding）设计的系统，与传统关系数据库不同，它专注于向量的特性和相似性。

    数据以向量形式存在，代表不同数据项，包括数字、文本、图像等。这类数据库利用高效索引和查询算法来优化向量数据的存储和检索。

    向量数据库通过计算与目标向量的余弦距离、点积等获取与目标向量的相似度。当处理大量甚至海量的向量数据时，向量数据库索引和查询算法的效率明显高于传统数据库。

2. 主流的向量数据库

    * [Chroma](https://www.trychroma.com/)：轻量级向量数据库，功能简单，易用，无GPU加速，适合初学者。
    * [Weaviate](https://weaviate.io/)：开源向量数据库，支持相似度搜索、MMR搜索和混合搜索，提高结果相关性和准确性。
    * [Qdrant](https://qdrant.tech/)：用Rust开发，检索效率高，支持本地和云端部署，可通过不同键复用数据。

## 使用Embedding API

这里选择了智谱API，Moonshot 不提供 NLP Embedding 服务

智谱API的调用方法可以[参考](../Code/ZhipuAIEmbeddingAPI.ipynb)

## 数据预处理

为构建我们的本地知识库，我们需要对本地的MD文档进行处理，读取本地文档并通过 Embedding 方法将文档的内容转化为词向量来构建向量数据库。

测试使用的知识库数据源存放在`../DataBase/KnowledgeDB`目录下

### 1.读取MD文档

[参考 Code/DataProcessing.ipynb 代码块1](../Code/DataProcessing.ipynb)

使用`langchain.document_loaders.markdown`中的`UnstructuredMarkdownLoader`对 Markdown 进行读取，在Jupyter的注释中注明了所需的依赖包和其他一些事项

### 2.数据清洗

[参考 Code/DataProcessing.ipynb 代码块2](../Code/DataProcessing.ipynb)

使用`re`正则库或者`replace`函数对文档中的无关内容进行移除

### 3.文档分隔

由于单个文档的长度往往会超过模型支持的上下文，导致检索得到的知识太长超出模型的处理能力，因此，在构建向量知识库的过程中，我们通常将单个文档按长度或者按固定的规则分割成若干个 chunk，然后将每个 chunk 转化为词向量，存储到向量数据库中。

在检索时，我们会以 chunk 作为检索的元单位，也就是每一次检索到 k 个 chunk 作为模型可以参考来回答用户问题的知识，这个 k 是我们可以自由设定的。

Langchain 中文本分割器都根据 `chunk_size` (块大小)和 `chunk_overlap` (块与块之间的重叠大小)进行分割。

![alt text](../figure/0x03_Figure_02_DocumentSplit.png)

其中：

* chunk_size 指每个块包含的字符或 Token （如单词、句子等）的数量
* chunk_overlap 指两个块之间共享的字符数量，用于保持上下文的连贯性，避免分割丢失上下文信息

Langchain 提供多种文档分割方式，区别在怎么确定块与块之间的边界、块由哪些字符/token组成、以及如何测量块大小

|分隔器名称                                | 描述                                                |
|-----------------------------------------|----------------------------------------------------- |
| RecursiveCharacterTextSplitter()        | 按字符串分割文本，递归地尝试按不同的分隔符进行分割文本。|
| CharacterTextSplitter()                 | 按字符来分割文本。                                    |
| MarkdownHeaderTextSplitter()            | 基于指定的标题来分割markdown文件。                     |
| TokenTextSplitter()                     | 按token来分割文本。                                   |
| SentenceTransformersTokenTextSplitter() | 按token来分割文本。                                   |
| Language()                              | 用于CPP、Python、Ruby、Markdown等。                   |
| NLTKTextSplitter()                      | 使用NLTK（自然语言工具包）按句子分割文本。              |
| SpacyTextSplitter()                     | 使用Spacy按句子的切割文本。                            |

[参考 Code/DataProcessing.ipynb 代码块3](../Code/DataProcessing.ipynb)
