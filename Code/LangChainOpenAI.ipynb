{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "代码块1\n",
    "导入环境变量和基础配置\n",
    "\"\"\"\n",
    "\n",
    "# 在原版 OpenAI.ipynb 的基础上修改, 使用 LangChain 调用 API\n",
    "\n",
    "import os\n",
    "\n",
    "# pip install python-dotenv\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# find_dotenv() 寻找并定位 .env 文件的路径\n",
    "# load_dotenv() 读取该 .env 文件，并将其中的环境变量加载到当前的运行环境中\n",
    "# 确保每次读取后的参数能够覆盖旧的值, 保证修改能够生效\n",
    "\n",
    "_ = load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# 配置系统代理\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "# os.environ[\"HTTP_PROXY\"] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "代码块2\n",
    "使用 langchain 调用 LLM 的 API\n",
    "\"\"\"\n",
    "\n",
    "# 对原版 OpenAI API 来说, Kimi 是直接兼容的\n",
    "# 对于 LangChain 来说, 则不能使用 OpenAI 的 API, 社区提供了解决方案\n",
    "\n",
    "\"\"\"\n",
    "使用 langchain_openai\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.llms.moonshot import Moonshot\n",
    "\n",
    "api_key = os.environ.get(\"KIMI_API_KEY\")\n",
    "os.environ[\"moonshot_api_key\"] = api_key\n",
    "\n",
    "llm = Moonshot(temperature=0.0)\n",
    "# model_name max_tokens temperature stream 超参数可选\n",
    "# https://platform.moonshot.cn/docs/guide/utilize-the-streaming-output-feature-of-kimi-api\n",
    "# stream=True\n",
    "\n",
    "output = llm.invoke(\"你好，简单介绍下Moonshot\")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "代码块3\n",
    "构建 Prompt\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"你是一个翻译助手，可以帮助我将 {input_language} 翻译成 {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "text = \"我带着比身体重的行李，\\\n",
    "游入尼罗河底，\\\n",
    "经过几道闪电 看到一堆光圈，\\\n",
    "不确定是不是这里。\\\n",
    "\"\n",
    "messages  = chat_prompt.format_messages(input_language=\"中文\", output_language=\"英文\", text=text)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "代码块4\n",
    "输出解析器的使用\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "代码块5\n",
    "使用 LCEL 构建完整流程链\n",
    "\"\"\"\n",
    "\n",
    "# 类似于Linux的管道, 用法和效果都很像\n",
    "chain = chat_prompt | llm | output_parser\n",
    "chain.invoke({\"input_language\":\"中文\", \"output_language\":\"英文\",\"text\": text})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
